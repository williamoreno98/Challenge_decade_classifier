{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac77837a",
   "metadata": {},
   "source": [
    "### NLP challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "312c8cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26349307435121794\n",
      "F1 macro: 0.2538069741230763\n",
      "\n",
      "Reporte por clase:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         150       0.70      0.72      0.71       157\n",
      "         151       0.35      0.71      0.47       162\n",
      "         152       0.53      0.52      0.52       157\n",
      "         153       0.38      0.58      0.46       155\n",
      "         154       0.47      0.58      0.52       166\n",
      "         155       0.34      0.25      0.29       167\n",
      "         156       0.31      0.39      0.34       158\n",
      "         157       0.25      0.26      0.26       166\n",
      "         158       0.22      0.26      0.24       156\n",
      "         159       0.26      0.39      0.31       160\n",
      "         160       0.10      0.07      0.08       170\n",
      "         161       0.16      0.13      0.14       157\n",
      "         162       0.15      0.15      0.15       162\n",
      "         163       0.17      0.14      0.15       166\n",
      "         164       0.23      0.17      0.19       161\n",
      "         165       0.13      0.12      0.12       163\n",
      "         166       0.11      0.06      0.08       156\n",
      "         167       0.20      0.20      0.20       166\n",
      "         168       0.23      0.20      0.21       164\n",
      "         169       0.16      0.14      0.15       154\n",
      "         170       0.17      0.34      0.23       167\n",
      "         171       0.20      0.24      0.22       163\n",
      "         172       0.21      0.18      0.20       169\n",
      "         173       0.32      0.29      0.30       160\n",
      "         174       0.27      0.17      0.21       161\n",
      "         175       0.26      0.15      0.19       163\n",
      "         176       0.24      0.23      0.23       151\n",
      "         177       0.21      0.18      0.19       156\n",
      "         178       0.30      0.27      0.28       166\n",
      "         179       0.25      0.23      0.24       162\n",
      "         180       0.23      0.20      0.22       165\n",
      "         181       0.29      0.37      0.33       159\n",
      "         182       0.27      0.23      0.25       162\n",
      "         183       0.19      0.13      0.15       159\n",
      "         184       0.27      0.21      0.24       160\n",
      "         185       0.18      0.14      0.16       161\n",
      "         186       0.16      0.14      0.15       155\n",
      "         187       0.22      0.20      0.21       157\n",
      "         188       0.26      0.35      0.30       162\n",
      "\n",
      "    accuracy                           0.26      6281\n",
      "   macro avg       0.25      0.26      0.25      6281\n",
      "weighted avg       0.25      0.26      0.25      6281\n",
      "\n",
      "Modelo guardado en baseline_tfidf_logreg.joblib\n"
     ]
    }
   ],
   "source": [
    "# baseline_simple.py\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "import joblib\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# ---------- 1) Cargar datos ----------\n",
    "df = pd.read_csv(\"train.csv\")   # debe tener columnas \"text\" y \"decade\"\n",
    "df = df.dropna(subset=[\"text\", \"decade\"]).reset_index(drop=True)\n",
    "\n",
    "# Si decade viene como string o necesita conversión, conviértelo a int:\n",
    "# (según tu enunciado, 202 -> 2020; aquí mantendremos la etiqueta tal como está para el modelo)\n",
    "df['decade'] = df['decade'].astype(int)\n",
    "\n",
    "# ---------- 2) Preprocesamiento simple ----------\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", str(text))\n",
    "    # 2. Pasar a minúsculas\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "\n",
    "df['text_clean'] = df['text'].apply(clean_text)\n",
    "\n",
    "# ---------- 3) Train / Test split (estratificado) ----------\n",
    "X = df['text_clean'].values\n",
    "y = df['decade'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ---------- 4) Pipeline (TF-IDF word + char) + LogisticRegression ----------\n",
    "word_tfidf = TfidfVectorizer(ngram_range=(1,2), analyzer='word', max_df=0.9, min_df=2)\n",
    "char_tfidf = TfidfVectorizer(ngram_range=(3,5), analyzer='char', max_df=0.9, min_df=2)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([('word', word_tfidf), ('char', char_tfidf)])),\n",
    "    ('clf', LogisticRegression(max_iter=2000, class_weight='balanced', solver='saga', random_state=42))\n",
    "])\n",
    "\n",
    "# ---------- 5) Entrenar ----------\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# ---------- 6) Evaluar ----------\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 macro:\", f1_score(y_test, y_pred, average='macro'))\n",
    "print(\"\\nReporte por clase:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ---------- 7) Guardar el modelo entrenado ----------\n",
    "joblib.dump(pipeline, \"baseline_tfidf_logreg.joblib\")\n",
    "print(\"Modelo guardado en baseline_tfidf_logreg.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6acc8577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en baseline_tfidf_logreg_full.joblib\n"
     ]
    }
   ],
   "source": [
    "# baseline_simple.py\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import unicodedata\n",
    "\n",
    "# ---------- 1) Cargar datos ----------\n",
    "df = pd.read_csv(\"train.csv\")   # debe tener columnas \"text\" y \"decade\"\n",
    "df = df.dropna(subset=[\"text\", \"decade\"]).reset_index(drop=True)\n",
    "\n",
    "# Convertir decade a int (si viene como string)\n",
    "df['decade'] = df['decade'].astype(int)\n",
    "\n",
    "# ---------- 2) Preprocesamiento simple ----------\n",
    "def clean_text(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", str(text))\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df['text_clean'] = df['text'].apply(clean_text)\n",
    "\n",
    "X = df['text_clean'].values\n",
    "y = df['decade'].values\n",
    "\n",
    "# ---------- 3) Pipeline (TF-IDF word + char) + LogisticRegression ----------\n",
    "word_tfidf = TfidfVectorizer(ngram_range=(1,2), analyzer='word', max_df=0.9, min_df=2)\n",
    "char_tfidf = TfidfVectorizer(ngram_range=(3,5), analyzer='char', max_df=0.9, min_df=2)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([('word', word_tfidf), ('char', char_tfidf)])),\n",
    "    ('clf', LogisticRegression(max_iter=2000, class_weight='balanced', solver='saga', random_state=42))\n",
    "])\n",
    "\n",
    "# ---------- 4) Entrenar con TODO ----------\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# ---------- 5) Guardar el modelo entrenado ----------\n",
    "joblib.dump(pipeline, \"baseline_tfidf_logreg_full.joblib\")\n",
    "print(\"Modelo guardado en baseline_tfidf_logreg_full.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332d2492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de predicciones guardado en submission.csv\n"
     ]
    }
   ],
   "source": [
    "# predict_eval.py\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import unicodedata\n",
    "\n",
    "# ---------- 1) Cargar modelo entrenado ----------\n",
    "pipeline = joblib.load(\"baseline_tfidf_logreg_full.joblib\")\n",
    "\n",
    "# ---------- 2) Cargar datos de evaluación ----------\n",
    "df_eval = pd.read_csv(\"eval.csv\")  # columnas: id,text\n",
    "\n",
    "# ---------- 3) Preprocesamiento (mismo que en entrenamiento) ----------\n",
    "def clean_text(text: str) -> str:\n",
    "    text = unicodedata.normalize(\"NFKC\", str(text))\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df_eval['text_clean'] = df_eval['text'].apply(clean_text)\n",
    "\n",
    "# ---------- 4) Predecir ----------\n",
    "X_eval = df_eval['text_clean'].values\n",
    "preds = pipeline.predict(X_eval)\n",
    "\n",
    "# ---------- 5) Guardar archivo de respuesta ----------\n",
    "df_out = pd.DataFrame({\n",
    "    \"id\": df_eval[\"id\"],\n",
    "    \"answer\": preds\n",
    "})\n",
    "\n",
    "df_out.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Archivo de predicciones guardado en submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
